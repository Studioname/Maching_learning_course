{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529718f8",
   "metadata": {},
   "source": [
    "# Week 7 - Linear Regression\n",
    "\n",
    "## Machine Learning Algorithms - Types and Tasks\n",
    "\n",
    "### Types of Machine Learning Algorithms\n",
    "\n",
    "There are three main types of ML algo\n",
    "\n",
    "1. Regression [e.g linear regression]\n",
    "2. Classification [e.g k-nearest-neighbour]\n",
    "3. Clustering [e.g k-means]\n",
    "\n",
    "#### Regression\n",
    "\n",
    "Regression is a **supervised** learning technique to **predict numerical quantities**. This might be something like **predicting the value of a particular stock**.\n",
    "\n",
    "Regression algorithms in ML include **linear regression and generalised linear regression** [also called multivariate analysis in traditional statistics]. Logistic regression is mostly used as a classification algorithm. \n",
    "\n",
    "#### Classification\n",
    "\n",
    "Classification is also a **supervised** learning technique, for **predicting numeric or categorical quantities**. An example might be an image classifier, which tells you if a picture is of a certain object or not. In this case the data is already labelled, so we can compare the prediction with the label that was predicted for it.\n",
    "\n",
    "Classification algorithms include:\n",
    "\n",
    "1. Logistic regression [as mentioned earlier]\n",
    "2. Decision trees [a single tree]\n",
    "3. Random forests [multiple trees]\n",
    "4. K-NN [K-nearest neighbour]\n",
    "5. Naive Bayes\n",
    "6. Support Vector Machines [SVM]\n",
    "\n",
    "**Some ML algos like Support Vector Machines, random forests, and k-nearest-neighbour support regression as well as classification**. In the case of Support Vector Machines, the sci-kit implementation of this algo provides two apis - SVC for classification and SVR for regression. \n",
    "\n",
    "**A random forest consists of multiple independent trees**, and **each tree makes a prediction regarding the value of a feature.** If the feature is numeric, take the mean or the mode or perform some other calculation in order to determine the final prediction. If the feature is categorical, use the mode as the result. In event of a tie, select one in a random fashion (shouldn't it return \"both\"?)\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "Clustering is an **unsupervised** learning technique for grouping similar data together. \n",
    "\n",
    "**Clustering algos put data points in different clusters without knowing the nature of the data points. After the data has been separated into different clusters, you can use the Support Vector Machine algorithm to perform classification.**\n",
    "\n",
    "Clustering algorithms in ML include:\n",
    "\n",
    "1. K-means\n",
    "2. Hierarchical cluster analysis [HCA]\n",
    "3. Meanshift\n",
    "4. Expectation maximization\n",
    "\n",
    "**In k-means, the value of k is a hyperparameter - usually an odd number to avoid ties between two classes.**\n",
    "\n",
    "**The meanshift algorithm is a variation of k-means that does not require you to specify a value for k. It will determine the optimal value of clusters by itself, but it does not scale well for large datasets.**\n",
    "\n",
    "#### Machine Learning Tasks\n",
    "\n",
    "1. Obtain a dataset\n",
    "2. Clean data\n",
    "3. Feature engineering\n",
    "4. Dimensionality reduction\n",
    "5. Algorithm selection\n",
    "6. Prepare data\n",
    "7. Training the model\n",
    "8. Testing the model\n",
    "9. Fine-tuning the model\n",
    "10. Obtain metrics for the model\n",
    "\n",
    "**Unless you have a dataset that is already sanitised, you need to examine the data in a dataset to make sure it is suitable.**\n",
    "\n",
    "The **data preparation phase** involves:\n",
    "\n",
    "1. Examining the rows (data cleaning) to ensure they contain valid data\n",
    "2. Examining the columns (feature selection or extraction) to determine if you can retain only the most important columns.\n",
    "\n",
    "##### 1. Obtain a Dataset \n",
    "\n",
    "Get a relevant dataset.\n",
    "\n",
    "##### 2.  Clean the Data\n",
    "\n",
    "We can do this using the following techniques:\n",
    "\n",
    "1. Missing value ration\n",
    "2. Low variance filter\n",
    "3. High correlation filter\n",
    "\n",
    "Data cleaning mostly means checking the values in a dataset in order to resolve one or more of the following problems:\n",
    "\n",
    "1. Fix incorrect values\n",
    "2. Resolve duplicate values\n",
    "3. Resolve missing values\n",
    "4. Decide what to do with outliers\n",
    "\n",
    "If our dataset has too many **incorrect values**, we can use a **statistical value (e.g mean, mode etc)** to replace incorrect values with suitable ones. Duplicate values can be resolved in a similar fashion. \n",
    "\n",
    "To resolve **duplicate values**, we can use the **low variance technique** to **identify and drop features with constant values from the dataset**.\n",
    "\n",
    "If the dataset has **too many missing values**, we can **use the missing value ratio technique**. In extreme cases, we might be able to drop features with a large number of missing values.\n",
    "\n",
    "To **find highly correlated features which increase multicollinearity in the dataset**, we can use the **high correlation filter**. Such features can be removed from a dataset, but check with your domain expert before doing so. \n",
    "\n",
    "You can replace missing numeric values with zero, the minimum, the mean, the mode, or the maximum value in a numeric column. You can replace missing categorical values with the mode of the categorical column.\n",
    "\n",
    "If a row in a dataset contains a value that is an **outlier**, we have three choices:\n",
    "\n",
    "1. Delete the row\n",
    "2. Keep the row\n",
    "3. Replace the outlier with some other value (like mean, mode etc).\n",
    "\n",
    "When a dataset contains an outlier, we need to make a decision based on domain knowledge that is specific to the given dataset. Let's take the stock market crash of 1929 as an outlier. Such an occurrence is rare, and certainly an outlier, but can still contain meaningful information.\n",
    "\n",
    "##### 3. Feature Engineering\n",
    "\n",
    "**Feature Engineering** means to **examine the features in a dataset to determine whether or not you can reduce the dimensionality [e.g the number of columns]** of the dataset. The process for doing so involves **three main techniques**:\n",
    "\n",
    "1. Feature engineering\n",
    "2. Feature selection\n",
    "3. Feature extraction [aka feature projection]\n",
    "\n",
    "**Feature engineering** is the process of **creating a new set of features based on a combination of existing features in order to create a meaningful dataset for a given task**. Domain expertise is often required for this. After we have created a dataset, we perform **feature selection and/or extraction** to maximise quality.\n",
    "\n",
    "**Feature selection** is also called **variable selection, attribute selection or variable subset selection**. It involves **selecting a subset of the most relevant features in a dataset**, providing these advantages:\n",
    "\n",
    "1. Reduced training time\n",
    "2. Simpler models are easier to interpret\n",
    "3. Avoidance of the 'curse of dimensionality'\n",
    "4. Better generalisation due to reduction in overfitting [reduction in variance]\n",
    "\n",
    "Feature selection techniques are often used in domains where there are **many features and comparatively few samples (or data points)**. A low value feature can be redundant or irrelevant, which are different concepts. A relevant feature might be redundant when it is combined with another strongly correlated feature. For example, a relevant feature might be redundant when it is combined with another strongly correlated feature. \n",
    "\n",
    "**Feature selection can use three strategies**:\n",
    "\n",
    "1. The filter strategy (e.g information gain)\n",
    "2. The wrapper strategy (e.g search guided by accuracy)\n",
    "3. The embedded strategy (prediction errors are used to determine whether features are included or excluded while developing a model).\n",
    "\n",
    "**Feature selection can also be useful for regressions and classification tasks.**\n",
    "\n",
    "**Feature extraction creates new features from functions that produce combinations of the original features.** In contrast, feature selection involves selecting a subset of existing features. **Feature selection and extraction both reduce dimensionality**.\n",
    "\n",
    "##### 4. Dimensionality Reduction \n",
    "\n",
    "This refers to algorithms that reduce the number of features in a dataset. There are many techniques for this, and they **involve feature selection or extraction**. Dimensionality reduction algorithms **usually combine feature extraction and dimensionality reduction**, such as Principal Component Analysis (PCA).\n",
    "\n",
    "##### 5. Algorithm Selection\n",
    "\n",
    "We select the appropriate type of algorithm for our ML task [e.g linear regression, random forest, k-means cluster]. These are broadly covered earlier.\n",
    "\n",
    "##### 6. Prepare Data\n",
    "\n",
    "**At this point we are ready to split the dataset into two parts, training and test data**. The training set is used for training a model, and the test set is used for making predictions. \n",
    "\n",
    "**We must make sure our dataset conforms to the following guidelines:**\n",
    "\n",
    "1. The set is **large enough to yield results**.\n",
    "2. It is **representative of the dataset as a whole**\n",
    "3. **Never train on test data**\n",
    "4. **Never test on train data**\n",
    "\n",
    "###### Note on Cross Validation\n",
    "\n",
    "The purpose of **Cross Validation** is to **test a model with nonoverlapping test sets**. \n",
    "\n",
    "It is performed in the following manner:\n",
    "\n",
    "1. Split the data into k subsets of equal size\n",
    "\n",
    "2. Select one subset for testing and the others for training\n",
    "\n",
    "3. Repeat step 2 for the other subsets. \n",
    "\n",
    "This process is called **k-fold cross-validation, and the overall error estimate is the average of the error estimates.** A standard method for evaluation involves ten-fold cross-validation. 10 subsets is the optimal number. **You can repeat ten-fold cross-validation ten times and compute the average of the results, which reduces variance**.\n",
    "\n",
    "##### Note on Regularisation\n",
    "\n",
    "**Regularisation helps solve overfitting**, which occurs when **a model performs well on training data but poorly on validation or test data**. Regularisation solves this problem by **adding a penalty term to the cost function, thereby controlling the model complexity with this penalty term.** \n",
    "\n",
    "It is useful for:\n",
    "\n",
    "1. Large number of variables\n",
    "2. Low ratio of number of observations to number of variables\n",
    "3. High multicollinearity\n",
    "\n",
    "There are two main types of regularisation: **L1 regularisation**, related to **MAE, Mean Absolute Error**, or the **absolute value of differences**, and **L2 regularisation**, which is related to **MSE, or Mean Square Error**, or the **square of differences**.\n",
    "\n",
    "**In general, L2 performs better than L1 and L2 is more computationally efficient**. \n",
    "\n",
    "##### Note on Feature Scaling\n",
    "\n",
    "**Feature Scaling standardises the range of features of data**. This is performed during data pre-processing, in part because gradient descent benefits from it. The assumption is that the data conforms to a standard normal distribution, and standardisation involves subtracting the mean and dividing by the standard deviation for every data point, resulting in a N(0,1) normal distribution.\n",
    "\n",
    "##### Note on Bias\n",
    "\n",
    "**Bias can be due to an error from wrong assumptions in a learning algorithm.** \n",
    "\n",
    "**High bias** can **cause an algorithm** to **miss relevant relations between features and target outputs (underfitting)**. \n",
    "\n",
    "**Prediction bias can occur because of noisy data, an incomplete feature set, or a biased training sample** \n",
    "\n",
    "Errors caused by bias can make the difference between the average prediction of your model and the correct value you want to predict. \n",
    "\n",
    "**The solution** is to **repeat the model-building process multiple times, gather new data each time, and produce an analysis to produce a new model**. These **resulting models** have a **range of predictions** because the **underlying datasets have a degree of randomness**. \n",
    "\n",
    "**Bias measures the distance of the predictions from these models to the correct value**. \n",
    "\n",
    "##### Note on Variance\n",
    "\n",
    "**Variance** is the **expected value of the squared deviation from the mean**. \n",
    "\n",
    "**High variance** can or might **cause an algorithm** to **model the random noise in the training data, rather than the intended outputs (overfitting).** \n",
    "\n",
    "**Adding parameters to a model increases its complexity and variance, while decreasing the bias.**\n",
    "\n",
    "**Dealing with bias and variance is dealing with underfitting and overfitting**. Error due to variance is the variability of a model prediction for a given data point. \n",
    "\n",
    "**The solution is the same as for Bias, and the variance is the extent to which predictions for a given point vary among different instances of that model**.\n",
    "\n",
    "##### Note on Performance Metrics\n",
    "\n",
    "One of the most frequently used metrics is **R-squared**, which measures **how close the data is to the fitted regression line (regression coefficient)**. The R-squared value is always a percentage between 0 and 100%, indicating **how much the model explains the variability of the response data around its mean.** In general, a **higher R-squared value indicates a better model**.\n",
    "\n",
    "Although high R-squared values are preferred, they are not necessarily laways good values. Low R-squared values are not always bad. For example, an R-squared value for predicting human behaviour is often less than 50%. Also, **R-squared cannot determine whether the coefficient estimates and predictions are biased, or whether a regression model is adequate**. **Thus it is possible to have a low R-squared value for a good model, or a high R-squared value for a poorly fitting model.** **Evaluate R-squared values in conjunction with residual plots, other model statistics, and subject area knowledge**. \n",
    "\n",
    "##### Note on Confusion Matrices\n",
    "\n",
    "A confusion matrix is an n x n table that shows the number of false positives, false negatives, true positives and true negatives a model provides. **The diagonal values of a confusion matrix are correct, whereas off-diagonal values are incorrect.** In general, **a low false positive value is better than a false negative value**. \n",
    "\n",
    "A 2x2 confusion matrix has four possible values representing the combinations of correct and incorrect classifications. The definitions of **Precision, Accuracy and Recall** are given by the following formulas:\n",
    "\n",
    "- 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃/(𝑇𝑁 + 𝐹𝑃)\n",
    "- 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = (𝑇𝑃 + 𝑇𝑁)/[𝑃 + 𝑁]\n",
    "- 𝑅𝑒𝑐𝑎𝑙𝑙 = 𝑇𝑃/[𝑇𝑃 + 𝐹𝑁]\n",
    "\n",
    "**Accuracy can be unreliable as a metric because it yields misleading results in unbalanced datasets.**\n",
    "\n",
    "When the number of observations in different classes are significantly different, it gives equal importance to both false positive and false negative classifications. For example, declaring cancer as benign is worse than incorrectly telling people it's malign - Accuracy can't differentiate between these two cases. \n",
    "\n",
    "##### Note on Receiving Operating Characteristic (ROC)\n",
    "\n",
    "**The ROC curve is a curve that plots the true positive rate (TPR, i.e the recall) against the false positive rate (FPR).** Note that the true negative rate (TNR) is also called the specificity. The following links contains Python code using SKLearn and the Iris dataset, and also code for plotting the ROC:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-inpython\n",
    "\n",
    "##### Other Useful Statistical Terms\n",
    "\n",
    "Machine Learning relies on a number of statistical quantities in order to assess the validity of a model. Here are some examples:\n",
    "\n",
    "- RSS (Residual Sum of Squares)\n",
    "- TSS (Total Sum of Squares)\n",
    "- R2 (R-Squared)\n",
    "- F1 score\n",
    "- p-value\n",
    "\n",
    "The definition of RSS, TSS, and R-squared are shown in the following, where 𝑦̂  is the y-coordinate of a point on a best fitting line and 𝑦̅ is the mean of the y-values of the points in the dataset:\n",
    "\n",
    "- 𝑅𝑆𝑆 = 𝑠𝑢𝑚 𝑜𝑓 𝑠𝑞𝑢𝑎𝑟𝑒𝑠 𝑜𝑓 𝑟𝑒𝑠𝑖𝑑𝑢𝑎𝑙𝑠 = (𝑦 − 𝑦̂)*2\n",
    "- 𝑇𝑆𝑆 = 𝑡𝑜𝑎𝑙 𝑠𝑢𝑚 𝑜𝑓 𝑠𝑞𝑢𝑎𝑟𝑒𝑠 = (𝑦 − 𝑦̅)*2\n",
    "- 𝑅2 = 1 − 𝑅𝑆𝑆/𝑇𝑆𝑆\n",
    "\n",
    "**F1 Score**\n",
    "\n",
    "**The F1 score is a measure of the accuracy of a test, and is defined as the harmonic mean of precision and recall.** Here are the relevant formulas, where **p is the precision and r is the recall:**\n",
    "\n",
    "- 𝑝 = (# 𝑜𝑓 𝑐𝑜𝑟𝑟𝑒𝑐𝑡 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 𝑟𝑒𝑠𝑢𝑙𝑡𝑠)/(# 𝑜𝑓 𝑎𝑙𝑙 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 𝑟𝑒𝑠𝑢𝑙𝑡𝑠)\n",
    "- 𝑟 = (# 𝑜𝑓 𝑐𝑜𝑟𝑟𝑒𝑐𝑡 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 𝑟𝑒𝑠𝑢𝑙𝑡𝑠)/(# 𝑜𝑓 𝑎𝑙𝑙 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑠𝑎𝑚𝑝𝑙𝑒𝑠)\n",
    "- 𝐹1 − 𝑠𝑐𝑜𝑟𝑒 = 1/[((1/𝑟) + (1/𝑝))/2] = 2 × [𝑝 × 𝑟]/[𝑝 + 𝑟]\n",
    "\n",
    "An f1 score is considered **perfect when it is 1, and a total failure when it is 0**. Keep in mind that an f1 score tends to be **used to categorical classification problems**, while the **R2 value is typically for regression tasks**.\n",
    "\n",
    "**p-value**\n",
    "\n",
    "**The p-value is used to reject the null hypothesis if the p-value is small enough (<0.005) which indicates a higher significance**. The null hypothesis **states that there is no correlation between a dependent variable (such as y) and an independent variable (such as x)**. The **threshold value for p is typically between 1% and 5%.** There is no straightforward formula for calculating p-values, which are values that are always between 0 and 1. In fact, p-values are statistical quantities to evaluate the so-called null hypothesis, and they are calculated by means of p-value tables or via spredsheet/statistical software.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "In **supervised learning**, the first sub-category we learn about is **regression analysis**, that is, the **prediction of continuous outcomes**. In regression analysis, **we are given a number of predictor (explanatory) variables and a continuous response variable (outcome), and we try to find a relationship between those variables that allow us to predict an outcome**. In ML, **predictor variables are called 'features' and response variables are called 'target variables'**.\n",
    "\n",
    "Regression models are used to **predict target variables on a continuous scale**, making them attractive for addressing many questions in science. \n",
    "\n",
    "The **goal** of linear regression is to **model the relationship between feature(s) and a continuous target variable**. Regression analysis is used to **predict outputs on a continuous scale rather than categorical class labels**. \n",
    "\n",
    "#### Simple Linear Regression\n",
    "\n",
    "**The goal of simple (univariate) linear regression is to model the relationship between a single feature (explanatory variable x) and a continuous-valued target (response variable y)**. The equation of a linear model with one explanatory variable is defined as follows:\n",
    "\n",
    "𝑦  =   𝑤0 + 𝑤1𝑥\n",
    "\n",
    "Here, the **weight w0 represents the y axis intercept** and **w1 is the weight coefficient of the explanatory variable**, geometrically **representing the gradient of the regression line**. The **gradient and y-intercept are commonly referred to as the weight and the bias**. \n",
    "\n",
    "**Our goal is to learn the weights of the linear equation to describe the relationship between the explanatory variable and the target variable, which can then be used to predict the responses of new explanatory variables not part of the training dataset**.\n",
    "\n",
    "Based on the linear equation defined above, linear regression can be understood as **finding the best-fitting line straight through the training examples. The best-fitting line is also called the regression line**, and the vertical lines from the regression lines to the training examples are the 'offsets' or 'residuals' - the errors of our prediction.\n",
    "\n",
    "#### Multiple Linear Regression\n",
    "\n",
    "In simple linear regression, it has only one explanatory variable. We can generalise the simple linear regression model to multiple explanatory variables - This is called **multiple linear regression**. \n",
    "\n",
    "### Linear Regression in Python\n",
    "\n",
    "The weights and dependent variables of the regression line can be represented as vectors m = [m1, m2, m3] and x = [x1, x2, x3]. To consolidate in Python 3, the linear regression function for 1-dimensional X can be defined as using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04523e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import random\n",
    "\n",
    "def f(m, X, c):\n",
    "    \"\"\"Linear regression\"\"\"\n",
    "    return [m * x + c for x in X]\n",
    "X = [i for i in range(10)]\n",
    "y = [x + random.random() for x in X]\n",
    "m, c = 1, 0\n",
    "y_hat = f(m, X, c)\n",
    "plt.plot(X, y, '.',c='r')\n",
    "plt.plot(X, y_hat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4ab4f",
   "metadata": {},
   "source": [
    "The linear regression y = 1 * X  + 0,  where the gradient is m=1 and the y-intercept is c=0 seems to fit the data well but a quantifiable measure is required in order to compare between models with different values of m and c.\n",
    "\n",
    "The error of the model can be quantified in terms of the difference between y, the actual output, and y_hat, the prediction of the model. One error measure is **mean squared error**.\n",
    "\n",
    "**Mean Squared Error (MSE)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    c = 0\n",
    "    n = len(y)\n",
    "    for i in range(n):\n",
    "        c += (y_hat[i] - y[i]) ** 2\n",
    "    return 1/2 * c/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8afd8",
   "metadata": {},
   "source": [
    "**A lower mean squared error indicates parameters that result in a better fitting linear regression model.** The **goal of optimising a model** is to **minimise the error**. Therefore, **an optimisation algorithm**, such as gradient descent, **is\n",
    "required**.\n",
    "\n",
    "##### Gradient Descent\n",
    "\n",
    "Gradient descent is an **optimisation algorithm** for **finding the local minimum** of a **differentiable function**. The loss function:\n",
    "\n",
    "**MSE = (y_hat - y)**2** of a linear regression model may be minimised to improve model fitness.\n",
    "\n",
    "In the following diagram the vertical axis represents the MSE while the horizontal axis represents a weight, e.g m. By changing the value of m, the MSE may increase or decrease. To minimise MSE, m must move towards a direction where MSE decreases. \n",
    "\n",
    "https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.kdnuggets.com%2F2020%2F05%2F5-concepts-gradient-descent-cost-function.html&psig=AOvVaw2D-4fIEVbDUeaUf1xcY3xi&ust=1670334247978000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCNDQu43O4vsCFQAAAAAdAAAAABAE\n",
    "\n",
    "**The key step in gradient descent is the weight update, where the weights are changed according to the learning rate, a hyperparameter that determines how much weights change by. The bias, c, also changes in the same way**.\n",
    "\n",
    "Here is the gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba56c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: weight=1.05, bias=0.01\n",
      "Epoch 1: weight=1.07, bias=0.02\n",
      "Epoch 2: weight=1.08, bias=0.02\n",
      "Epoch 3: weight=1.08, bias=0.02\n",
      "Epoch 4: weight=1.08, bias=0.03\n",
      "Epoch 5: weight=1.08, bias=0.03\n",
      "Epoch 6: weight=1.08, bias=0.03\n",
      "Epoch 7: weight=1.08, bias=0.03\n",
      "Epoch 8: weight=1.08, bias=0.04\n",
      "Epoch 9: weight=1.08, bias=0.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "weight = 1\n",
    "bias = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = f(weight, X, bias)\n",
    "    n = len(y_hat)\n",
    "    \n",
    "    # compute partial derivatives of MSE w.r.t. m and c\n",
    "    \n",
    "    dm = (-2/n) * sum([X[i] * (y[i] - y_hat[i]) for i in range(n)])\n",
    "    dc = (-2/n) * sum([y[i] - y_hat[i] for i in range(n)])\n",
    "    \n",
    "    # update weight and bias\n",
    "    \n",
    "    weight = weight - learning_rate * dm\n",
    "    bias = bias - learning_rate * dc\n",
    "    print(f\"Epoch {i}: weight={weight:.2f}, bias={bias:.2f}\")\n",
    "    \n",
    "y_hat = f(weight, X, bias)\n",
    "plt.plot(X, y, ',', c='r')\n",
    "plt.plot(X,y_hat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750ff68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: m=1.05, c=0.01\n",
      "Epoch 1: m=1.07, c=0.02\n",
      "Epoch 2: m=1.08, c=0.02\n",
      "Epoch 3: m=1.08, c=0.02\n",
      "Epoch 4: m=1.08, c=0.03\n",
      "Epoch 5: m=1.08, c=0.03\n",
      "Epoch 6: m=1.08, c=0.03\n",
      "Epoch 7: m=1.08, c=0.03\n",
      "Epoch 8: m=1.08, c=0.04\n",
      "Epoch 9: m=1.08, c=0.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e, m, c, a = 10, 1, 0, 0.01\n",
    "for i in range(e):\n",
    "    y_hat = f(m, X, c)\n",
    "    n = len(y_hat)\n",
    "    # Compute partial derivatives of MSE w.r.t. m and c\n",
    "    dm = (-2/n) * sum([X[i] * (y[i] - y_hat[i]) for i in range(n)])\n",
    "    dc = (-2/n) * sum([y[i] - y_hat[i] for i in range(n)])\n",
    "    # Update weight and bias\n",
    "    m = m - a * dm\n",
    "    c = c - a * dc\n",
    "    print(f\"Epoch {i}: m={m:.2f}, c={c:.2f}\")\n",
    "    \n",
    "y_hat = f(m, X, c)\n",
    "plt.plot(X, y, '.', c='r')\n",
    "plt.plot(X, y_hat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68858199",
   "metadata": {},
   "source": [
    "In order to perform linear regression in python, all we have to do is apply the proper packages and their functions and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254b7ff",
   "metadata": {},
   "source": [
    "### Python Packages for Linear Regression\n",
    "\n",
    "The packages we will use are **NumPy** and **scikit-learn**, a widely  used Python library for machine learning built on Numpy and some other packages. It provides the means for pre-processing data, reducing dimensionality, implementing regression, classificiation, clustering etc. Like NumPy sci-kit learn is also open source. \n",
    "\n",
    "If we need to know how something in sci-kit learn works, we can check the documentation. \n",
    "\n",
    "If we want to implement linear regression and **need functionality beyond the scope of sci-kit learn, we should consider statsmodels**. It's a powerful python package for the estimation of statistical models, performing tests, and more.\n",
    "\n",
    "### Simple Linear Regression with Sci-kit Learn\n",
    "\n",
    "Let's start with the simplest case - simple linear regression.\n",
    "\n",
    "There are **five basic steps** to **implement linear regression**:\n",
    "\n",
    "1. Import the packages and classes we need\n",
    "\n",
    "2. Provide data to work with and eventually do appropriate transformations\n",
    "\n",
    "3. Create a regression model and fit it with our data\n",
    "\n",
    "4. Check the results of model fitting to know whether the model is sound\n",
    "\n",
    "5. Apply the model for predictions\n",
    "\n",
    "#### Step 1. Import Packages and Classes\n",
    "\n",
    "The first step is to import numpy and the class LinearRegression from sklearn.linear_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb3ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a22a51",
   "metadata": {},
   "source": [
    "Now we have all the functionalities we need to implement linear regression. \n",
    "\n",
    "**The fundamental data type of NumPy is the array type called numpy.ndarray, hereafter referred to as array**. \n",
    "\n",
    "**The class sklearn.linear_model.LinearRegression will be used to perform linear and polynomial regression and make predictions accordingly**. \n",
    "\n",
    "##### Step 2. Provide Data\n",
    "\n",
    "**Now we define some data to work with. The inputs (regressors, x) and output (predictor, y) should be arrays or similar objects.**\n",
    "\n",
    "Here is a simple way of providing some data for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0d43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5,15,25,35,45,55]).reshape((-1,1))\n",
    "y = np.array([5,20,14,32,22,38])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db4813",
   "metadata": {},
   "source": [
    "Now we have two arrays, the **input x and output y**. **We should call reshape() on x because this array is required to be two-dimensional, or to be more precise, to have one column and as many rows as necessary**. That's exactly what the argument (-1, 1) of reshape() specifies.\n",
    "\n",
    "This is how x and y look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8755bc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5],\n",
       "        [15],\n",
       "        [25],\n",
       "        [35],\n",
       "        [45],\n",
       "        [55]]),\n",
       " array([ 5, 20, 14, 32, 22, 38]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4125b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b31451b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0131563",
   "metadata": {},
   "source": [
    "As we can see, x has two dimensions, and x.shape is (6, 1), while y has a single dimension, and its shape is (6,).\n",
    "\n",
    "#### Step 3. Create a Model and Fit it\n",
    "\n",
    "**The next step is to create a linear regression model and fit it with our data.**\n",
    "\n",
    "Let's create an instance of the class LinearRegression, which represents the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a187b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23dd8df",
   "metadata": {},
   "source": [
    "There are several **optional parameters** to LinearRegression:\n",
    "\n",
    "- fit_intercept is a Boolean [True by default] that decides whether to calculate the intercept [True] or consider it equal to zero [false]\n",
    "- normalize is a Boolean [False by default] that decides whether to normalize the input variables [True] or not [False]\n",
    "- copy_X is a Boolean [True by default] that decides whether to copy [True] or overwrite the input variables [False]\n",
    "- n_jobs is an integer or None [default] and represents the number of jobs used in parallel computation. None usually means one job and -1 to use all processors.\n",
    "\n",
    "It's time to start using the model. **First, we need to call .fit() on our model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1e73a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72048415",
   "metadata": {},
   "source": [
    "**With .fit(), you calculate the optimal values of the weights b0 and b1 using the existing input and output (x and y) as the arguments. In other words, .fit() fits the model**. It returns itself, which in our case, is the variable 'model' itself. We cna therefore replace the last two statements with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c72218ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67a29d",
   "metadata": {},
   "source": [
    "#### Step 4. Get Results\n",
    "\n",
    "**Once we have our model fitted, we can get the results to check whether the model works satisfactorily and interpret it.**\n",
    "\n",
    "We can obtain the **coefficient of determination** (R2) by calling **.score()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52df7145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination: 0.715875613747954\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x,y)\n",
    "print('Coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8205b34",
   "metadata": {},
   "source": [
    "**When we're applying .score(), the arguments are also the predictor x and regressor y, and the return value is R-squared.**\n",
    "\n",
    "**The attributes of our model are .intercept_, which represents the coefficient b0 and .coeff_, which represents b1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f4bb508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 5.633333333333329\n"
     ]
    }
   ],
   "source": [
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "603a39b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: [0.54]\n"
     ]
    }
   ],
   "source": [
    "print('Slope:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ee128",
   "metadata": {},
   "source": [
    "The code above illustrates **how to get b0 and b1.** You can notice that **.intercept_ is a scalar, while .coef_ is an array.**\n",
    "\n",
    "The value b0 = 5.63 (approximately) illustrates that your model predicts the response 5.63 when x is zero. The value b1 = 0.54 means that the predicted response rises by 0.54 when x is increased by one.\n",
    "\n",
    "You should notice that **you can provide y as a two-dimensional array as well. In this case, we'll get a similar result**. This is how it might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a66065c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [5.63333333]\n"
     ]
    }
   ],
   "source": [
    "new_model = LinearRegression().fit(x, y.reshape((-1,1)))\n",
    "print('Intercept:', new_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cfb6c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: [[0.54]]\n"
     ]
    }
   ],
   "source": [
    "print('Slope:', new_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49666b6",
   "metadata": {},
   "source": [
    "This example is very similar to the previous one, but in this case **.intercept_** is a 1D array with the single element B0 and  **.coef_** is a 2D array with the single element B1.\n",
    "\n",
    "#### Step 5. Predict Response\n",
    "\n",
    "Once we have a satisfactory model, we can use it for predictions with either existing or new data.\n",
    "\n",
    "To obtain the predicted response, use **.predict()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c95dbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response:\n",
      "[[ 8.33333333]\n",
      " [13.73333333]\n",
      " [19.13333333]\n",
      " [24.53333333]\n",
      " [29.93333333]\n",
      " [35.33333333]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.intercept_ + model.coef_ * x\n",
    "print('Predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26707f5",
   "metadata": {},
   "source": [
    "In this case, remember that x is an array, and **we are multiplying each element of x with model.coef_ and adding model.intercept_ to the product.**\n",
    "\n",
    "The output here differs from the previous example only in dimensions. The predicted response is now a 2D array, while in the previous case it had only one dimension.\n",
    "\n",
    "**If we reduce the number of dimensions of x to one, these two approaches will yield the same result. We can do this by replacing x with x.reshape(-1), x.flatten(), or x.ravel() when multiplying it with the model.coef_**. \n",
    "\n",
    "In practice, regression models are often applied for forcecasts. **This means you can use fitted models to calculate the outputs based on some other, new inputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38261ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.arange(5).reshape((-1,1))\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f507a28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.63333333, 6.17333333, 6.71333333, 7.25333333, 7.79333333])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = model.predict(x_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2424695",
   "metadata": {},
   "source": [
    "**Here, .predict() is applied to the new regressor x_new, and yields the response y_new**. This example uses arange() from NumPy to generate an array with the elements from 0 (inclusive) up to the specified number.\n",
    "\n",
    "### Multiple Linear Regression with Scikit-Learn\n",
    "\n",
    "**You can implement multiple linear regression following the same steps as you would for simple regression.**\n",
    "\n",
    "#### Steps 1 and 2. Import Packages and Classes, Provide Data\n",
    "\n",
    "First we import numpy and sklearn.linear_model.LinearRegression and provide known inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e64d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [[0,1], [5,1], [15,2], [25,5], [35,11], [45,15], [55,34], [60,35]]\n",
    "y = [4,5,20,14,32,22,38,43]\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769c8e9",
   "metadata": {},
   "source": [
    "This is a simple way to define the input x and output y. We can print x and y to see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82265fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1],\n",
       "        [ 5,  1],\n",
       "        [15,  2],\n",
       "        [25,  5],\n",
       "        [35, 11],\n",
       "        [45, 15],\n",
       "        [55, 34],\n",
       "        [60, 35]]),\n",
       " array([ 4,  5, 20, 14, 32, 22, 38, 43]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5e99a",
   "metadata": {},
   "source": [
    "**In multiple linear regression, x is a 2D array with at least two columns, while y is usually a 1D array.** This is a simple example of multiple linear regression, and x has exactly two columns.\n",
    "\n",
    "##### Step 3. Create a Model and Fit\n",
    "\n",
    "The next step is to create an instance of LinearRegression and fit it with **.fit()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ff78d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6145918",
   "metadata": {},
   "source": [
    "#### Step 4. Get Results\n",
    "\n",
    "**You can obtain the properties of the model the same way you would for simple linear regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66235522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination: 0.8615939258756775\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x,y)\n",
    "print('Coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae550877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept 5.52257927519819\n"
     ]
    }
   ],
   "source": [
    "print('Intercept', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9aa81a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope [0.44706965 0.25502548]\n"
     ]
    }
   ],
   "source": [
    "print('Slope', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be773f",
   "metadata": {},
   "source": [
    "**You obtain the value of R-squared using .score()**\n",
    "\n",
    "**You obtain the values of the estimators of regression coefficients with .intercept_ and .coef_**\n",
    "\n",
    "**.intercept_ holds the bias b0 while now .coef_ is an array containing b1 and b2 respectively.**\n",
    "\n",
    "In this example, the intercept is approximately 5.52, and this is the value of the predicted response when x1 = x2 = 0. The increase of x1 by 1 yields the rise of the predicted response by 0.45. Similarly when x2 grows by 1, the response rises by 0.26.\n",
    "\n",
    "#### Step 5. Predict Response\n",
    "\n",
    "Predictions work the same way as in the case of simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c400e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('Predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7beaf42",
   "metadata": {},
   "source": [
    "**The predicted response is obtained with .predict()**, which is very similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "447f8678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.intercept_ + np.sum(model.coef_ * x, axis=1)\n",
    "print('Predicted response', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0112e",
   "metadata": {},
   "source": [
    "**You can predict the output values by multiplying each column of the input with the appropriate weight, summing the results and adding the intercept to the sum.**\n",
    "\n",
    "You can apply this model to new data as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a86549f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.arange(10).reshape((-1,2))\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f048d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.77760476,  7.18179502,  8.58598528,  9.99017554, 11.3943658 ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = model.predict(x_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73e22e",
   "metadata": {},
   "source": [
    "### Polynomial Regression with Scikit-Learn\n",
    "\n",
    "**Implementing polynomial regression with scikit-learn is very similar to linear regression. There is one extra step - To transform the array of inputs to include non-linear terms such as x2**. \n",
    "\n",
    "#### Step 1. Import Packages and Classes\n",
    "\n",
    "In addition to numpy and sklearn.linear_model.LinearRegression, **we should also import the class PolynomialFeatures from sklearn.preprocessing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5621f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c381f0",
   "metadata": {},
   "source": [
    "#### Step 2a. Provide Data\n",
    "\n",
    "This step defines the input and output and is the same in the case of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13e50f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5,15,25,35,45,55]).reshape((-1,1))\n",
    "y = np.array([15,11,2,8,25,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a2e86",
   "metadata": {},
   "source": [
    "Now we have the input and ouput in a suitable format. **Keep in mind we need the input to be a 2D array - That's why .reshape() is used.**\n",
    "\n",
    "##### Step 2b. Transform Input Data\n",
    "\n",
    "This is the **new step** we need to implement for polynomial regression. As seen earlier, **we need to include x2 and perhaps other additional features when implementing polynomial regression.** For that reason, we transform the input array x to contain the additional column(s) with the values x2 (and eventually more features).\n",
    "\n",
    "It's possible to transform the input array in several ways (like using **.insert()** from numpy), but the class PolynomialFeatures is very convenient for this purpose. Let's create an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b5231a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de31b6c",
   "metadata": {},
   "source": [
    "The variable transformer refers to an instance of **PolynomialFeatures** which you can use to transform the input x.\n",
    "\n",
    "You can provide several optional parameters to **PolynomialFeatures**:\n",
    "\n",
    "- **degree** is an interger (2 by default) that represents the degree of the polynomial regression function\n",
    "\n",
    "- **interaction_only** is a Boolean [False by default] that decides whether to include only interaction features [True] or all features [False]\n",
    "\n",
    "- **include_bias** is a Boolean [True by default] that decides whether to include the bias [intercept] column of ones [True] or not [False]\n",
    "\n",
    "This example uses the default values of all parameters, but sometimes you will want to experiment with the degree of the function, and it can be beneficial to provide this argument anyway.\n",
    "\n",
    "Before applying transformer, we need to **fit it with .fit()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a28222b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolynomialFeatures(include_bias=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9838590",
   "metadata": {},
   "source": [
    "**Once our transformer is fitted, it's ready to create a new, modified input. To do this, apply .transform()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45c6ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = transformer.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca87fee",
   "metadata": {},
   "source": [
    "**This is the transformation of the input array with .transform(). It takes the input array as the argument and returns the modified array.**\n",
    "\n",
    "**You can also use .fit_transform() to replace the three previous statements with only one:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6deed6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.,   25.],\n",
       "       [  15.,  225.],\n",
       "       [  25.,  625.],\n",
       "       [  35., 1225.],\n",
       "       [  45., 2025.],\n",
       "       [  55., 3025.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c3a84",
   "metadata": {},
   "source": [
    "**The modified input array contains two columns: One with the original inputs and the other with their squares.**\n",
    "\n",
    "#### Step 3: Create a Model and Fit it\n",
    "\n",
    "This step is also the same as in the case of linear regression. You create and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b16205e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c91ca",
   "metadata": {},
   "source": [
    "The regression model is now created and fitted. It's ready for application.\n",
    "\n",
    "You should keep in mind that **the first argument of .fit() is the modified input array x_, and not the original x**. \n",
    "\n",
    "#### Step 4. Get Results\n",
    "\n",
    "**You can obtain the properties of the model the same way as in the case of linear regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb022652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination: 0.8908516262498564\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print('Coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "570b0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 21.372321428571425\n"
     ]
    }
   ],
   "source": [
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f36bb3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-1.32357143  0.02839286]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52fd95",
   "metadata": {},
   "source": [
    "Again, **.score() returns R-squared**. Its first argument is also the modified input x_, not x. The values of the weights are associated to **.intercept_ and .coef_**. **.intercept_ represents b0, while .coef_ references the array that contains b1 and b2 respectively.**\n",
    "\n",
    "You can obtain a very similar result with different transformation and regression elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f89f7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = PolynomialFeatures(degree=2, include_bias=True).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73baea",
   "metadata": {},
   "source": [
    "If you call **PolynomialFeatures** with the default parameter **include_bias=True** you'll obtain the new **input array x_ with the additional leftmost column containing only ones**. This column corresponds to the intercept. This is how the modified input array looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21527201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 5.000e+00, 2.500e+01],\n",
       "       [1.000e+00, 1.500e+01, 2.250e+02],\n",
       "       [1.000e+00, 2.500e+01, 6.250e+02],\n",
       "       [1.000e+00, 3.500e+01, 1.225e+03],\n",
       "       [1.000e+00, 4.500e+01, 2.025e+03],\n",
       "       [1.000e+00, 5.500e+01, 3.025e+03]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb3c24",
   "metadata": {},
   "source": [
    "The **first column contains ones**, the **second has the values of x**, and the **third has the squares of x.** The **intercept is already included with the leftmost column**, and you **don't need to include it again when creating the instance of LinearRegression**. **Thus you can provide fit_intercept=False.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d46a5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(x_,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c27eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination: 0.8908516262498565\n",
      "Intercept: 0.0\n",
      "Coefficients [21.37232143 -1.32357143  0.02839286]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print('Coefficient of determination:', r_sq)\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Coefficients', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ea3f0",
   "metadata": {},
   "source": [
    "**We can see that .intercept_ is zero, but .coef_ contains b0 as its first element. Everything else is the same.**\n",
    "\n",
    "#### Step 5. Predict Response\n",
    "\n",
    "**Now we use .predict()** to get the predicted response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28d5459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response:\n",
      "[15.46428571  7.90714286  6.02857143  9.82857143 19.30714286 34.46428571]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_)\n",
    "print('Predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b51a0",
   "metadata": {},
   "source": [
    "As we can see, **the prediction works almost the same way as in linear regression. It just requires the modified input instead of the original**. \n",
    "\n",
    "**We can apply an identical procedure if we have several input variables. This would give us an input array with more than one column, but everything else would be the same. Here is an example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fbb45d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Import packages\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2a. Provide data\n",
    "\n",
    "x = [[0,1], [5,1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8069b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b: Transform input data\n",
    "\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "\n",
    "r_sq = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d379d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict\n",
    "\n",
    "y_pred = model.predict(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b5383",
   "metadata": {},
   "source": [
    "This yields the following results and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50a87b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination 0.9453701449127822\n",
      "Intercept: 0.8430556452395734\n",
      "Coefficients:\n",
      "[ 2.44828275  0.16160353 -0.15259677  0.47928683 -0.4641851 ]\n",
      "Predicted response:\n",
      "[ 0.54047408 11.36340283 16.07809622 15.79139    29.73858619 23.50834636\n",
      " 39.05631386 41.92339046]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient of determination', r_sq)\n",
    "print('Intercept:', intercept)\n",
    "print('Coefficients:', coefficients, sep='\\n')\n",
    "print('Predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd3848",
   "metadata": {},
   "source": [
    "In this case there are six regression coefficients (including the intercept) as shown in the estimated regression function:\n",
    "\n",
    "𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂ + 𝑏₃𝑥₁² + 𝑏₄𝑥₁𝑥₂ + 𝑏₅𝑥₂²\n",
    "\n",
    "We can also notice that **polynomial regression yielded a higher coefficient of determination than multiple linear regression for the same problem**. **At first we might think a large R-squared is a good result. It might be. However, having a complex model and R-squared very close to 1 might also be a sign of overfitting. To check the performance of a model**, we should **test it with new data**, that is **with observations not used to fit (train) the model.**\n",
    "\n",
    "### Advanced Linear Regression with Statsmodels\n",
    "\n",
    "You can **implement linear regression in Python** relatively **easily** by using the **package statsmodels**. This is typically desirable **when there is a need for more detailed results**. The procedure is similar to that of scikit-learn.\n",
    "\n",
    "#### Step 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb602df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c937d5",
   "metadata": {},
   "source": [
    "#### Step 2. Provide Data and Transform Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6144a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34],[60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745a539",
   "metadata": {},
   "source": [
    "The input and output arrays are created, but **we still need to add the column of ones to the inputs if we want statsmodels to calculate the intercept b0. It doesn't take b0 into account by default**. This is just one function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "667788ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922dc95",
   "metadata": {},
   "source": [
    "This is how we add the column of ones to x, with **add_constant()**. It takes the input array x as an argument and returns a new array with the column of ones inserted at the beginning. This is how x and y look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30d10723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  0.,  1.],\n",
       "        [ 1.,  5.,  1.],\n",
       "        [ 1., 15.,  2.],\n",
       "        [ 1., 25.,  5.],\n",
       "        [ 1., 35., 11.],\n",
       "        [ 1., 45., 15.],\n",
       "        [ 1., 55., 34.],\n",
       "        [ 1., 60., 35.]]),\n",
       " array([ 4,  5, 20, 14, 32, 22, 38, 43]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753404c",
   "metadata": {},
   "source": [
    "We can see that **the modified x has three columns**: The **first column of ones [corresponding to b0 and replacing the intercept]**, as well as the **two columns of the original features**.\n",
    "\n",
    "#### Step 3. Create a Model and Fit it\n",
    "\n",
    "**The regression model based on ordinary least squares is an instance of the class statsmodels.regression.linear_model.OLS**. This is how we create an object of that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5fa2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7947d",
   "metadata": {},
   "source": [
    "**We must be careful here. The first argument is the output, followed by the input. There are several more optional parameters.**\n",
    "\n",
    "**Once our model is created, we can apply .fit() on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2af469ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fcf3f8",
   "metadata": {},
   "source": [
    "**By calling .fit(), we obtain the variable results, which is an instance of the class statsmodels.regression.linear_model.RegressionResultsWrapper. This object holds a lot of information about the regression model.**\n",
    "\n",
    "#### Step 4. Get Results\n",
    "\n",
    "The variable results refer to the object that contains detailed information about the results of linear regression. **You can call .summary() to get a table with the results of your linear regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8264a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conan\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1541: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.56</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 05 Dec 2022</td> <th>  Prob (F-statistic):</th>  <td>0.00713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:20:19</td>     <th>  Log-Likelihood:    </th> <td> -24.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     8</td>      <th>  AIC:               </th> <td>   54.63</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     5</td>      <th>  BIC:               </th> <td>   54.87</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    5.5226</td> <td>    4.431</td> <td>    1.246</td> <td> 0.268</td> <td>   -5.867</td> <td>   16.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.4471</td> <td>    0.285</td> <td>    1.567</td> <td> 0.178</td> <td>   -0.286</td> <td>    1.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2550</td> <td>    0.453</td> <td>    0.563</td> <td> 0.598</td> <td>   -0.910</td> <td>    1.420</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.561</td> <th>  Durbin-Watson:     </th> <td>   3.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.755</td> <th>  Jarque-Bera (JB):  </th> <td>   0.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.380</td> <th>  Prob(JB):          </th> <td>   0.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.987</td> <th>  Cond. No.          </th> <td>    80.1</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.862\n",
       "Model:                            OLS   Adj. R-squared:                  0.806\n",
       "Method:                 Least Squares   F-statistic:                     15.56\n",
       "Date:                Mon, 05 Dec 2022   Prob (F-statistic):            0.00713\n",
       "Time:                        17:20:19   Log-Likelihood:                -24.316\n",
       "No. Observations:                   8   AIC:                             54.63\n",
       "Df Residuals:                       5   BIC:                             54.87\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          5.5226      4.431      1.246      0.268      -5.867      16.912\n",
       "x1             0.4471      0.285      1.567      0.178      -0.286       1.180\n",
       "x2             0.2550      0.453      0.563      0.598      -0.910       1.420\n",
       "==============================================================================\n",
       "Omnibus:                        0.561   Durbin-Watson:                   3.268\n",
       "Prob(Omnibus):                  0.755   Jarque-Bera (JB):                0.534\n",
       "Skew:                           0.380   Prob(JB):                        0.766\n",
       "Kurtosis:                       1.987   Cond. No.                         80.1\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cfcf3",
   "metadata": {},
   "source": [
    "This table is very comprehensive. In it we can find many statistical values assocated with linear regression including R-squared, b0, b1 and b2.\n",
    "\n",
    "We also see the **error warning kurtosistest**. This is **due to the small number of observations provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb9bfb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination: 0.8615939258756777\n",
      "Adjusted coefficient of determination 0.8062314962259488\n",
      "Regression coefficients: [5.52257928 0.44706965 0.25502548]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient of determination:', results.rsquared)\n",
    "print('Adjusted coefficient of determination', results.rsquared_adj)\n",
    "print('Regression coefficients:', results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1992833",
   "metadata": {},
   "source": [
    "As we can see,\n",
    "\n",
    "- **.rsquared** holds R-squared\n",
    "- **.rsquared_adj** represents adjusted R-squared [which is to say, R-squared corrected according to the number of input features]\n",
    "- **.params** refers the array with b0, b1, b2 respectively\n",
    "\n",
    "**As we can see, these results are identical to those obtained with scikit-learn for the same problem.**\n",
    "\n",
    "#### Step 5. Predict Response\n",
    "\n",
    "**You can obtain the predicted response on the input values used for creating the model using .fittedvalues or .predict() with the input array as the argument.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef0f8a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n",
      "Predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted response:', results.fittedvalues, sep='\\n')\n",
    "print('Predicted response:', results.predict(x), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfbe47",
   "metadata": {},
   "source": [
    "**This is the predicted response for known inputs.** If you **want predictions with new regressors**, you can also **apply .predict() with the new data as the argument**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a1fac48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 4., 5.],\n",
       "       [1., 6., 7.],\n",
       "       [1., 8., 9.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = sm.add_constant(np.arange(10).reshape((-1,2)))\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "080b25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.77760476,  7.18179502,  8.58598528,  9.99017554, 11.3943658 ])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = results.predict(x_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c8325",
   "metadata": {},
   "source": [
    "As we can see, the predicted results are the same as those obtained with scikit-learn for the same problem.\n",
    "\n",
    "### Beyond Linear Regression\n",
    "\n",
    "**Linear regression is sometimes not appropriate, especially for non-linear models of high complexity.**\n",
    "\n",
    "Fortunately there are other regression techniques available for these cases. Some of them are Support Vector Machines, decision trees, random forest, and neural networks. There are numerous Python libraries for regression using these techniques. Most of them are free and open source. \n",
    "\n",
    "The package scikit-learn provides the means for using other regression techniques in a very similar way to what we've seen. It contains the classes for all of these techniques, with the methods **.fit()**, **predict()**, etc.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "**We now know what linear regression is and how to implement it in NumPy, scikit-learn and statsmodels.**\n",
    "\n",
    "We use numpy for handling arrays\n",
    "\n",
    "Linear regression is **implemented with scikit-learn if we don't need detailed results, otherwise we use statsmodels.**\n",
    "\n",
    "Both approaches are worth learning more about and exploring further.\n",
    "\n",
    "When performing linear regression in Python, we perform these steps:\n",
    "\n",
    "1. Import packages and classes we need\n",
    "2. Provide data to work with, clean said data and then maybe transform it\n",
    "3. Create a regression model and fit it with our data\n",
    "4. Check the results of model fitting to find out whether it is satisfactory\n",
    "5. Apply the model for predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
